# Beacon

## Requirements
* iOS 15+
* XCode 13+
* ARKit 5
* RealityKit 2

**Note:** This app cannot be run in the XCode simulator and needs an actual device to run due to ARKit

**Note:** Recommended to use with an iPhone 12 or 13 with LIDAR as these devices will have improved AR Experiences. Additionally, phones provide a better usability than iPads due to their cellular connectivity, therefore greater GPS accuracy. 

---

## Installation
1. Clone GitHub source code
2. Get the [Alan iOS SDK framework](https://alan.app/docs/client-api/ios/ios-api/#step-1-get-the-alan-ios-sdk-framework)
   1. Download and extract the AlanSDK.xcframework.zip file from the latest release and extract AlanSDK.xcframework
   2. Move the AlanSDK.xcframework to the root of the Xcode project.
   3. In the General tab, under the Frameworks -> Libraries -> Embedded Content section, select Embed & Sign for the AlanSDK.xcframework
3. Select the Beacon App top layer -> Targets (Beacon) -> Signing & Capabilities -> Choose a provisonal profile

## Source Files 
Below is our file structure for the Core Beacon App. The top level folders are organized as follows

### ğŸ“ 3D Assets & Assets
Our 3D models and thumbnails that are packaged into the application 

### ğŸ“ Managers & ViewModels
In the MVVM architecture, these folders were for our View Models that managed our application data and state 

### ğŸ“ Models
The Models in our MVVM architecture. This was used to create a custom model class that loaded our 
3D models

### ğŸ“ Resources
Our custom Hand Pose Classifcation model

### ğŸ“ Views
The views and subviews of our application

### Complete Tree Structure

```
Beacon
 â”£ 3Dassets
 â”£ Assets.xcassets
 â”£ Extensions
 â”ƒ â”— View+Extensions.swift
 â”£ Managers
 â”ƒ â”£ ARSessionManager
 â”ƒ â”ƒ â”£ ARSessionManager.swift
 â”ƒ â”ƒ â”£ ARSessionManagerAnchors.swift
 â”ƒ â”ƒ â”£ ARSessionManagerDelegate.swift
 â”ƒ â”ƒ â”— ARSessionManagerGestures.swift
 â”ƒ â”£ LocationBasedManager.swift
 â”ƒ â”£ ModelDeletionManager.swift
 â”ƒ â”— PlacementSettings.swift
 â”£ Models
 â”ƒ â”— Model.swift
 â”£ Resources
 â”ƒ â”— HandPoseClassification.mlmodel
 â”£ ViewModels
 â”ƒ â”£ MapViewModel.swift
 â”ƒ â”— ModelsViewModel.swift
 â”£ Views
 â”ƒ â”£ Map
 â”ƒ â”ƒ â”£ MapContainer.swift
 â”ƒ â”ƒ â”— MapView.swift
 â”ƒ â”£ ARViewContainer.swift
 â”ƒ â”£ AlanView.swift
 â”ƒ â”£ BrowseView.swift
 â”ƒ â”£ CollectionView.swift
 â”ƒ â”£ ContentView.swift
 â”ƒ â”£ ControlView.swift
 â”ƒ â”£ CustomARView.swift
 â”ƒ â”£ DeletionView.swift
 â”ƒ â”— PlacementView.swift
 â”£ BeaconApp.swift
 â”— ScenePersistenceHelper.swift
```

Utilizing Alan AI SDK, we had to create a custom view controller that layered on top of the Beacon app so that way we could implement the universal microphone. We also needed to implement custom configuration code so it could work with our existing Swift UI Components

```
 Alan
 â”£ AlanSDK.framework
 â”£ AlanSDKButtonState+CustomStringConvertible.swift
 â”£ ObjectAssociation.swift
 â”£ UIApplication+Alan.swift
 â”£ UIApplication+KeyWindow.swift
 â”£ View+OnLoad.swift
 â”— ViewDidLoadModifier.swift
```